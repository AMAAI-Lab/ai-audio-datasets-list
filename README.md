# AI-Audio-Datasets-List (ADL)

This is a dataset of speech, music and sound effects that can provide training data for AIGC, AI model training, intelligent audio tool development, and audio applications. The audio dataset is mainly used in speech recognition, speech synthesis, singing voice synthesis, music information retrieval, music generation, audio processing, sound synthesis, etc

* [Speech](#s)
* [Music](#m)
* [Sound Effect](#se)

## <span id="s">Speech</span>

* [LibriSpeech](http://www.openslr.org/12) - The **LibriSpeech** corpus is a collection of approximately 1,000 hours of audiobooks that are a part of the LibriVox project. Most of the audiobooks come from the Project Gutenberg. The training data is split into 3 partitions of 100hr, 360hr, and 500hr sets while the dev and test data are split into the ’clean’ and ’other’ categories, respectively, depending upon how well or challening Automatic Speech Recognition systems would perform against. Each of the dev and test sets is around 5hr in audio length.
* [AISHELL-1](http://www.openslr.org/33/) - AISHELL-1 is a corpus for speech recognition research and building speech recognition systems for Mandarin.
* [CN-Celeb](http://openslr.org/82/) - CN-Celeb is a large-scale speaker recognition dataset collected `in the wild'. This dataset contains more than 130,000 utterances from 1,000 Chinese celebrities, and covers 11 different genres in real world.
* [AVSpeech](https://looking-to-listen.github.io/avspeech/) - **AVSpeech** is a large-scale audio-visual dataset comprising speech clips with no interfering background signals. The segments are of varying length, between 3 and 10 seconds long, and in each clip the only visible face in the video and audible sound in the soundtrack belong to a single speaking person. In total, the dataset contains roughly 4700 hours of video segments with approximately 150,000 distinct speakers, spanning a wide variety of people, languages and face poses.
* [WenetSpeech](https://wenet.org.cn/WenetSpeech/) - WenetSpeech is a multi-domain Mandarin corpus consisting of 10,000+ hours high-quality labeled speech, 2,400+ hours weakly labelled speech, and about 10,000 hours unlabeled speech, with 22,400+ hours in total. The authors collected the data from YouTube and Podcast, which covers a variety of speaking styles, scenarios, domains, topics, and noisy conditions. An optical character recognition (OCR) based method is introduced to generate the audio/text segmentation candidates for the YouTube data on its corresponding video captions.
* [WSJ0-2mix](https://www.merl.com/demos/deep-clustering) - **WSJ0-2mix** is a speech recognition corpus of speech mixtures using utterances from the Wall Street Journal (WSJ0) corpus.
* [LibriTTS](http://www.openslr.org/60) - **LibriTTS** is a multi-speaker English corpus of approximately 585 hours of read English speech at 24kHz sampling rate, prepared by Heiga Zen with the assistance of Google Speech and Google Brain team members. The LibriTTS corpus is designed for TTS research. It is derived from the original materials (mp3 audio files from LibriVox and text files from Project Gutenberg) of the LibriSpeech corpus.
* [WHAM! (WSJ0 Hipster Ambient Mixtures)](http://wham.whisper.ai/) - The **WSJ0 Hipster Ambient Mixtures** ( **WHAM!** ) dataset pairs each two-speaker mixture in the wsj0-2mix dataset with a unique noise background scene. The noise audio was collected at various urban locations throughout the San Francisco Bay Area in late 2018. The environments primarily consist of restaurants, cafes, bars, and parks. Audio was recorded using an Apogee Sennheiser binaural microphone on a tripod between 1.0 and 1.5 meters off the ground.
* [VoxPopuli](https://github.com/facebookresearch/voxpopuli) - VoxPopuli is a large-scale multilingual corpus providing 100K hours of unlabelled speech data in 23 languages. It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16 languages and their aligned oral interpretations into 5 other languages totaling 5.1K hours.
* [FPT Open Speech Dataset (FOSD) ](https://data.mendeley.com/datasets/k9sxg2twv4/4) - This dataset consists of 25,921 recorded Vietnamese speeches (with their transcripts and the labelled start and end times of each speech) manually compiled from 3 sub-datasets (approximately 30 hours in total) released publicly in 2018 by FPT Corporation.
* [The People’s Speech](https://github.com/mlcommons/peoples-speech) - **The People's Speech** is a free-to-download 30,000-hour and growing supervised conversational English speech recognition dataset licensed for academic and commercial usage under CC-BY-SA (with a CC-BY subset). The data is collected via searching the Internet for appropriately licensed audio data with existing transcriptions.
* [ESD (Emotional Speech Database)](https://hltsingapore.github.io/ESD/) - **ESD** is an Emotional Speech Database for voice conversion research. The ESD database consists of 350 parallel utterances spoken by 10 native English and 10 native Chinese speakers and covers 5 emotion categories (neutral, happy, angry, sad and surprise). More than 29 hours of speech data were recorded in a controlled acoustic environment. The database is suitable for multi-speaker and cross-lingual emotional voice conversion studies.
* [Microsoft Speech Corpus (Indian languages)](https://msropendata.com/datasets/7230b4b1-912d-400e-be58-f84e0512985e) - Microsoft Speech Corpus (Indian languages) release contains conversational and phrasal speech training and test data for Telugu, Tamil and Gujarati languages. The data package includes audio and corresponding transcripts. Data provided in this dataset shall not be used for commercial purposes. You may use the data solely for research purposes. If you publish your findings, you must provide the following attribution: “Data provided by Microsoft and SpeechOcean.com”.

## <span id="m">Music</span>

* [MUSAN](http://www.openslr.org/17/) - **MUSAN** is a corpus of music, speech and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises.
* [MAESTRO](https://magenta.tensorflow.org/datasets/maestro) - The **MAESTRO** dataset contains over 200 hours of paired audio and MIDI recordings from ten years of International Piano-e-Competition. The MIDI data includes key strike velocities and sustain/sostenuto/una corda pedal positions. Audio and MIDI files are aligned with ∼3 ms accuracy and sliced to individual musical pieces, which are annotated with composer, title, and year of performance. Uncompressed audio is of CD quality or higher (44.1–48 kHz 16-bit PCM stereo).
* [MusicNet](https://zenodo.org/record/5120004#.Y56UPnZBw7d) - MusicNet is a collection of 330 freely-licensed classical music recordings, together with over 1 million annotated labels indicating the precise time of each note in every recording, the instrument that plays each note, and the note's position in the metrical structure of the composition. The labels are acquired from musical scores aligned to recordings by dynamic time warping. The labels are verified by trained musicians; we estimate a labeling error rate of 4%. We offer the MusicNet labels to the machine learning and music communities as a resource for training models and a common benchmark for comparing results.
* [jazznet](https://github.com/tosiron/jazznet) - jazznet: A Dataset of Fundamental Piano Patterns for Music Audio Machine Learning Research. This paper introduces the jazznet Dataset, a dataset of fundamental jazz piano music patterns for developing machine learning (ML) algorithms in music information retrieval (MIR). The dataset contains 162520 labeled piano patterns, including chords, arpeggios, scales, and chord progressions with their inversions, resulting in more than 26k hours of audio and a total size of 95GB.
* [The Lakh MIDI Dataset](https://colinraffel.com/projects/lmd/) - The Lakh MIDI dataset is a collection of 176,581 unique MIDI files, 45,129 of which have been matched and aligned to entries in the Million Song Dataset. Its goal is to facilitate large-scale music information retrieval, both symbolic (using the MIDI files alone) and audio content-based (using information extracted from the MIDI files as annotations for the matched audio files).
* [MTG-Jamendo](https://mtg.github.io/mtg-jamendo-dataset/) - The **MTG-Jamendo** dataset is an open dataset for music auto-tagging. The dataset contains over 55,000 full audio tracks with 195 tags categories (87 genre tags, 40 instrument tags, and 56 mood/theme tags). It is built using music available at Jamendo under Creative Commons licenses and tags provided by content uploaders. All audio is distributed in 320kbps MP3 format.
* [YM2413-MDB](https://jech2.github.io/YM2413-MDB/) - **YM2413-MDB** is an 80s FM video game music dataset with multi-label emotion annotations. It includes 669 audio and MIDI files of music from Sega and MSX PC games in the 80s using YM2413, a programmable sound generator based on FM. The collected game music is arranged with a subset of 15 monophonic instruments and one drum instrument.
* [JS Fake Chorales](https://github.com/omarperacha/js-fakes) - A MIDI dataset of 500 4-part chorales generated by the KS_Chorus algorithm, annotated with results from hundreds of listening test participants, with 300 further unannotated chorales.
* [Opencpop](https://wenet.org.cn/opencpop/) - **Opencpop** , a publicly available high-quality Mandarin singing corpus, is designed for singing voice synthesis (SVS) systems. This corpus consists of  **100 unique Mandarin songs** , which were recorded by a  **professional female singer.** All audio files were recorded with studio-quality at a sampling rate of **44,100 Hz** in a  **professional recording studio environment**.
* [ChMusic](https://github.com/haoranweiutd/chmusic) - **ChMusic** is a traditional Chinese music dataset for training model and performance evaluation of musical instrument recognition. This dataset cover 11 musical instruments, consisting of Erhu, Pipa, Sanxian, Dizi, Suona, Zhuiqin, Zhongruan, Liuqin, Guzheng, Yangqin and Sheng.
* [ComMU](https://pozalabs.github.io/ComMU/) - ComMU has 11,144 MIDI samples that consist of short note sequences created by professional composers with their corresponding 12 metadata. This dataset is designed for a new task, combinatorial music generation which generate diverse and high-quality music only with metadata through auto-regressive language model.
* [Nlakh](https://github.com/minju0821/musical_instrument_retrieval) - **Nlakh** is a dataset for Musical Instrument Retrieval. It is a combination of the NSynth dataset, which provides a large number of instruments, and the Lakh dataset, which provides multi-track MIDI data.
* [NSynth](https://magenta.tensorflow.org/datasets/nsynth) - **NSynth** is a dataset of one shot instrumental notes, containing 305,979 musical notes with unique pitch, timbre and envelope. The sounds were collected from 1006 instruments from commercial sample libraries and are annotated based on their source (acoustic, electronic or synthetic), instrument family and sonic qualities. The instrument families used in the annotation are bass, brass, flute, guitar, keyboard, mallet, organ, reed, string, synth lead and vocal. Four second monophonic 16kHz audio snippets were generated (notes) for the instruments.
* [Groove (Groove MIDI Dataset)](https://magenta.tensorflow.org/datasets/groove) - The **Groove MIDI Dataset (GMD)** is composed of 13.6 hours of aligned MIDI and (synthesized) audio of human-performed, tempo-aligned expressive drumming. The dataset contains 1,150 MIDI files and over 22,000 measures of drumming.
* [Music Data Sharing Platform for Computational Musicology Research (CCMUSIC DATASET)](https://zenodo.org/record/5676893#.Y7kuQXZBw7c) - This platform is a multi-functional music data sharing platform for Computational Musicology research.  It contains many music datas such as the sound information of Chinese traditional musical instruments and the labeling information of Chinese pop music, which is available for free use by computational musicology researchers.
* [Main Dataset for &#34;Evolution of Popular Music: USA 1960–2010&#34;](https://figshare.com/articles/dataset/Main_Dataset_for_Evolution_of_Popular_Music_USA_1960_2010_/1309953) - This is a large file (~20MB) called EvolutionPopUSA_MainData.csv, in comma-separated data format with column headers. Each row corresponds to a recording. The file is viewable in any text editor, and can also be opened in Excel or imported to other data processing programs.
* [The Italian Music Dataset](https://search.datacite.org/works/10.5281/zenodo.1298556) - The dataset is built by exploiting the Spotify and SoundCloud APIs. It is composed of over 14,500 different songs of both famous and less famous Italian musicians. Each song in the dataset is identified by its Spotify id and its title. Tracks' metadata include also lemmatized and POS-tagged lyrics and, in the most of cases, ten musical features directly gathered from Spotify. Musical features include acousticness (float), danceability (float), duration_ms (int), energy (float), instrumentalness (float), liveness (float), loudness (float), speechiness (float), tempo (float) and valence (float).
* [Music Topics and Metadata](https://data.mendeley.com/datasets/3t9vbwxgr5/1) - This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. We also provide some informations as lyrics which can be used to natural language processing.
* [MTG-Jamendo Dataset](https://github.com/MTG/mtg-jamendo-dataset) - We present the MTG-Jamendo Dataset, a new open dataset for music auto-tagging. It is built using music available at Jamendo under Creative Commons licenses and tags provided by content uploaders. The dataset contains over 55,000 full audio tracks with 195 tags from genre, instrument, and mood/theme categories. We provide elaborated data splits for researchers and report the performance of a simple baseline approach on five different sets of tags: genre, instrument, mood/theme, top-50, and overall.

## <span id="se">Sound Effect</span>

* [AudioSet](https://research.google.com/audioset/index.html) - Audioset is an audio event dataset, which consists of over 2M human-annotated 10-second video clips. These clips are collected from YouTube, therefore many of which are in poor-quality and contain multiple sound-sources. A hierarchical ontology of 632 event classes is employed to annotate these data, which means that the same sound could be annotated as different labels. For example, the sound of barking is annotated as Animal, Pets, and Dog. All the videos are split into Evaluation/Balanced-Train/Unbalanced-Train set.
* [UrbanSound8K](https://urbansounddataset.weebly.com/urbansound8k.html) - Urban Sound 8K is an audio dataset that contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy. All excerpts are taken from field recordings uploaded to www.freesound.org.
* [ESC-50](https://github.com/karolpiczak/ESC-50) - The **ESC-50** dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. It comprises 2000 5s-clips of 50 different classes across natural, human and domestic sounds, again, drawn from Freesound.org.
* [FSD50K (Freesound Database 50K)](https://zenodo.org/record/4060432#.Y56abXZBw7c) - Freesound Dataset 50k (or **FSD50K** for short) is an open dataset of human-labeled sound events containing 51,197 Freesound clips unequally distributed in 200 classes drawn from the AudioSet Ontology. FSD50K has been created at the Music Technology Group of Universitat Pompeu Fabra. It consists mainly of sound events produced by physical sound sources and production mechanisms, including human sounds, sounds of things, animals, natural sounds, musical instruments and more.
* [AudioCaps](https://audiocaps.github.io/) - **AudioCaps** is a dataset of sounds with event descriptions that was introduced for the task of audio captioning, with sounds sourced from the AudioSet dataset. Annotators were provided the audio tracks together with category hints (and with additional video hints if needed).
* [FSDnoisy18k](http://www.eduardofonseca.net/FSDnoisy18k/) - The **FSDnoisy18k** dataset is an open dataset containing 42.5 hours of audio across 20 sound event classes, including a small amount of manually-labeled data and a larger quantity of real-world noisy data. The audio content is taken from Freesound, and the dataset was curated using the Freesound Annotator. The noisy set of FSDnoisy18k consists of 15,813 audio clips (38.8h), and the test set consists of 947 audio clips (1.4h) with correct labels. The dataset features two main types of label noise: in-vocabulary (IV) and out-of-vocabulary (OOV). IV applies when, given an observed label that is incorrect or incomplete, the true or missing label is part of the target class set. Analogously, OOV means that the true or missing label is not covered by those 20 classes.
* [Knocking Sound Effects With Emotional Intentions](https://zenodo.org/record/3668503#.Y7kv9HZBw7c) - The dataset was recorded by the professional foley artist Ulf Olausson at the FoleyWorks studios in Stockholm on the 15th October, 2019. Inspired by previous work on knocking sounds. we chose five type of emotions to be portrayed in the dataset: anger, fear, happiness, neutral and sadness.
* [Pitch Audio Dataset (Surge synthesizer)](https://zenodo.org/record/4677097#.Y7kwVXZBw7c) - 3.4 hours of audio synthesized using the open-source [Surge synthesizer](https://surge-synthesizer.github.io/), based upon 2084 presets included in the Surge package. These represent ``natural'' synthesis sounds---i.e.presets devised by humans. We generated 4-second samples playing at velocity 64 with a note-on duration of 3 seconds. For each preset, we varied only the pitch, from MIDI 21--108, the range of a grand piano. Every sound in the dataset was RMS-level normalized using the [normalize](https://github.com/kklobe/normalize) package. There was no elegant way to dedup this dataset; however only a small percentage of presets (like drums and sound effects) had no perceptual pitch variation or ordering.

## And more
